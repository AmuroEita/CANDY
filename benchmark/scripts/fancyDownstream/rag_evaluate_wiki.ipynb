{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the tutorial to show how to evaluate the RAG's accuracy metrics by using haystack's key value store. We have a customized retriver and special domain knowledge, create the RAG under haystack pipeline, and  evaluate the RAG by using a packed evaluation class. The results are saved in standarlized csv files, for the convinience of both human and machine.\n",
    "- Dataset: wiki news from ockl\n",
    "- Metrics: bleu and rouge, for both the ending results and intermediate prompts (by using haystack's K-V in evaluator classs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to load wiki data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sarah Kambe Holland graduates from University of Central Florida.\n",
      "Milan Živadinović is the citizen of Serbia.\n",
      "Lior Refaelov represents Israel in the sport.\n",
      "Joyskim Dawa plays for FC Mariupol.\n",
      "Nedum Onuoha plays for Real Salt Lake.\n",
      "Corsican Assembly is headed by Jean-Guy Talamoni.\n",
      "Mahammed Dionne holds the political office of Prime Minister of Senegal.\n",
      "Carol Black holds the political office of principal.\n",
      "Alberto Felice De Toni holds the political office of rector.\n",
      "Bø Station is in the local government area of Bø.\n",
      "Sarah Kambe Holland graduates from what?\n",
      "Milan Živadinović is the citizen of what?\n",
      "Lior Refaelov represents what in the sport?\n",
      "Joyskim Dawa plays for what?\n",
      "Nedum Onuoha plays for what?\n",
      "Corsican Assembly is headed by what?\n",
      "Mahammed Dionne holds the political office of what?\n",
      "Carol Black holds the political office of what?\n",
      "Alberto Felice De Toni holds the political office of what?\n",
      "Bø Station is in the local government area of what?\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def extract_fields(filename, n):\n",
    "    queries = []\n",
    "    names = []\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        for i, line in enumerate(file):\n",
    "            if i >= n:\n",
    "                break\n",
    "            try:\n",
    "                #print(line)\n",
    "                data = json.loads(line)\n",
    "                query = data.get('query', 'No query field')\n",
    "                answer_names = [answer.get('name', 'No name field') for answer in data.get('answer', [])]\n",
    "                queries.append(query)\n",
    "                names.append(answer_names[0])\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Error decoding JSON on line {i + 1}\")\n",
    "    \n",
    "    return queries,names\n",
    "\n",
    "def replace_x_with_values(strings, replacements):\n",
    "    # This function replaces '_X_' in each string with corresponding values from the replacements list.\n",
    "    if len(strings) != len(replacements):\n",
    "        raise ValueError(\"The length of the strings list and the replacements list must be the same.\")\n",
    "    \n",
    "    processed_strings = []\n",
    "    placeholder = '_X_'\n",
    "    \n",
    "    for string, replacement in zip(strings, replacements):\n",
    "        # Replace '_X_' with the corresponding value from the replacements list\n",
    "        #print(replacement)\n",
    "        processed_string = string.replace(placeholder, replacement)\n",
    "        processed_strings.append(processed_string)\n",
    "    \n",
    "    return processed_strings\n",
    "# Example usage:\n",
    "filename = 'ockl_dataset_compact/datesorted_test_no_redundancy.jsonl'\n",
    "n = 10  # Number of lines you want to process\n",
    "queries,names = extract_fields(filename, n)\n",
    "corpus= replace_x_with_values(queries,names)\n",
    "for ct in corpus:\n",
    "    print(ct)\n",
    "def replace_x_with_what(strings):\n",
    "    # This function replaces '_X_' in each string with corresponding values from the replacements list.\n",
    "    \n",
    "    \n",
    "    processed_strings = []\n",
    "    placeholder = '_X_'\n",
    "    \n",
    "    for string in (strings):\n",
    "        # Replace '_X_' with the corresponding value from the replacements list\n",
    "        #print(replacement)\n",
    "        processed_string = string.replace(placeholder, 'what')\n",
    "        processed_string = processed_string.replace('.', '?')\n",
    "        processed_strings.append(processed_string)\n",
    "    \n",
    "    return processed_strings\n",
    "# Example usage:\n",
    "filename = 'ockl_dataset_compact/datesorted_test_no_redundancy.jsonl'\n",
    "n = 10  # Number of lines you want to process\n",
    "queries,names = extract_fields(filename, n)\n",
    "corpus= replace_x_with_values(queries,names)\n",
    "whats = replace_x_with_what(queries)\n",
    "for ct in whats:\n",
    "    print(ct)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Init rag components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tony/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "2024-04-18 16:21:21,044\tINFO util.py:159 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "/home/tony/.local/lib/python3.10/site-packages/torch/_utils.py:832: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "from fastrag.prompters.invocation_layers.llama_cpp import LlamaCPPInvocationLayer\n",
    "from haystack import Pipeline\n",
    "from haystack.nodes.prompt import PromptNode\n",
    "from haystack.nodes import PromptModel\n",
    "from haystack.nodes.prompt.prompt_template import PromptTemplate\n",
    "from haystack.nodes import AnswerParser\n",
    "from haystack.nodes.ranker import SentenceTransformersRanker\n",
    "from haystack.nodes.retriever import BM25Retriever\n",
    "from haystack.document_stores import InMemoryDocumentStore\n",
    "from haystack import Document\n",
    "from haystack import Pipeline\n",
    "from haystack.nodes.prompt import PromptNode\n",
    "from haystack.nodes import PromptModel\n",
    "from haystack.nodes.prompt.prompt_template import PromptTemplate\n",
    "from haystack.nodes import AnswerParser\n",
    "from haystack.nodes.ranker import SentenceTransformersRanker\n",
    "from haystack.nodes.retriever import BM25Retriever\n",
    "from haystack.document_stores import InMemoryDocumentStore\n",
    "from haystack import Document\n",
    "from warthunder2 import warthunderRead\n",
    "from CANDYRetriever2 import CANDYRetriever\n",
    "reranker = SentenceTransformersRanker(\n",
    "    batch_size= 32,\n",
    "    model_name_or_path= \"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
    "    top_k= 1,\n",
    "    use_gpu= False\n",
    ")\n",
    "AParser = AnswerParser()\n",
    "AParser = AnswerParser()\n",
    "LFQA = PromptTemplate(\n",
    "    prompt=\"\"\"{join(documents)}\n",
    "Question: {query}\n",
    "Answer: \"\"\",\n",
    "    output_parser= AParser\n",
    ")\n",
    "PrompterModel = PromptModel(\n",
    "    model_name_or_path= 'openlm-research/open_llama_3b',\n",
    "    model_kwargs={ 'max_new_token':32}\n",
    ")\n",
    "Prompter = PromptNode(\n",
    "    model_name_or_path= PrompterModel,\n",
    "    default_prompt_template= LFQA\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Optional) Another way for the prompt part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LFQA = PromptTemplate(\n",
    "    prompt=\"\"\"Given that {join(documents)}\n",
    "{query}\"\"\",\n",
    "    output_parser= AParser\n",
    ")\n",
    "Prompter = PromptNode(\n",
    "    model_name_or_path= PrompterModel,\n",
    "    default_prompt_template= LFQA\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Form the haystack pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.weight', 'question_encoder.bert_model.pooler.dense.bias']\n",
      "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:Thu Apr 18 16:22:44 2024:/home/tony/projects/CANDY/src/CANDY/FlatAMMIPObjIndex.cpp:24|virtual bool CANDY::FlatAMMIPObjIndex::setConfig(INTELLI::ConfigMapPtr)|\u001b[34mSize of DCO=-1\u001b[0m\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 22, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 1.57 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline()\n",
    "cfgFile = {\n",
    "    \"bleuBrevityPenalty\":1\n",
    "}\n",
    "retriever = CANDYRetriever(top_k=1,custom_rania_name='warthunder')\n",
    "retriever.setConfig(cfgFile)\n",
    "pipe.add_node(component=retriever, name= 'Retriever',inputs= [\"Query\"])\n",
    "pipe.add_node(component=reranker, name= 'Reranker',inputs= [\"Retriever\"])\n",
    "pipe.add_node(component=Prompter, name= 'Prompter',inputs= [\"Reranker\"])\n",
    "rubbishStr = 'hahahahahahahahahahaha'\n",
    "retriever.insertContext(rubbishStr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An end-to-end call of this rag to get answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization for 40, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.04 s\n",
      "Answer: \n",
      "Sarah Kambe Holland graduates from what? Sarah Kambe Holland graduates from what? Sarah Kambe Holland graduates from what? Sarah Kambe Holland graduates from what? Sarah Kambe Holland graduates from what? Sarah Kambe Holland graduates from what? Sarah Kambe Holland graduates from what? Sarah Kambe Holland graduates from what? Sarah Kambe Holland graduates from what? Sarah Kambe Holland graduates from what? Sarah Kambe Holland graduates from what? Sarah Kambe Holland graduates from what? Sarah\n"
     ]
    }
   ],
   "source": [
    "answer_result = pipe.run(whats[0],params={\n",
    "    \"Retriever\": {\n",
    "        \"top_k\": 1\n",
    "    },\n",
    "    \"Reranker\": {\n",
    "        \"top_k\": 1\n",
    "    },\n",
    "    \"generation_kwargs\":{\n",
    "        \"do_sample\": False,\n",
    "        \"max_new_tokens\": 128\n",
    "    }\n",
    "})\n",
    "print(f\"Answer: {answer_result['answers'][0].answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation: without the insert knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.weight', 'question_encoder.bert_model.pooler.dense.bias']\n",
      "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:Thu Apr 18 16:24:37 2024:/home/tony/projects/CANDY/src/CANDY/FlatAMMIPObjIndex.cpp:24|virtual bool CANDY::FlatAMMIPObjIndex::setConfig(INTELLI::ConfigMapPtr)|\u001b[34mSize of DCO=-1\u001b[0m\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 22, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.03 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 22, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.03 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 22, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.03 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 22, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.03 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 22, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.03 s\n",
      "I can evaluate bleu and rougle\n",
      "Tokenization for 40, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.03 s\n",
      "get ans [<Answer {'answer': '\\nSarah Kambe Holland graduates from what? Sarah Kambe Holland graduates from what? Sarah Kambe Holland graduates from what? Sarah Kambe Holland graduates from what? Sarah Kambe Holland graduates from what? Sarah Kambe Holland graduates from what? Sarah Kambe Holland graduates from what? Sarah Kambe Holland graduates from what? Sarah Kambe Holland graduates from what? Sarah Kambe Holland graduates from what? Sarah Kambe Holland graduates from what? Sarah Kambe Holland graduates from what? Sarah', 'type': 'generative', 'score': None, 'context': None, 'offsets_in_document': None, 'offsets_in_context': None, 'document_ids': ['ef34c98806178eb6ad09a938562c1763'], 'meta': {'prompt': 'Given that hahahahahahahahahahaha\\nSarah Kambe Holland graduates from what?'}}>]\n",
      "Tokenization for 41, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.03 s\n",
      "get ans [<Answer {'answer': '\\nMilan Živadinović is the citizen of Serbia.\\nMilan Živadinović is the citizen of Serbia and Montenegro.\\nMilan Živadinović is the citizen of Serbia and Montenegro.\\nMilan Živadinović is the citizen of Serbia and Montenegro.\\nMilan Živadinović is the citizen of', 'type': 'generative', 'score': None, 'context': None, 'offsets_in_document': None, 'offsets_in_context': None, 'document_ids': ['ef34c98806178eb6ad09a938562c1763'], 'meta': {'prompt': 'Given that hahahahahahahahahahaha\\nMilan Živadinović is the citizen of what?'}}>]\n",
      "done query\n",
      "[\"<Answer: answer='\\nSarah Kambe Holland graduates from what? Sarah Kambe Holland graduates from what? Sarah Kambe Holland graduates from what? Sarah Kambe Holland graduates from what? Sarah Kambe Holland graduates from what? Sarah Kambe Holland graduates from what? Sarah Kambe Holland graduates from what? Sarah Kambe Holland graduates from what? Sarah Kambe Holland graduates from what? Sarah Kambe Holland graduates from what? Sarah Kambe Holland graduates from what? Sarah Kambe Holland graduates from what? Sarah', score=None, context=None>\", \"<Answer: answer='\\nMilan Živadinović is the citizen of Serbia.\\nMilan Živadinović is the citizen of Serbia and Montenegro.\\nMilan Živadinović is the citizen of Serbia and Montenegro.\\nMilan Živadinović is the citizen of Serbia and Montenegro.\\nMilan Živadinović is the citizen of', score=None, context=None>\"]\n",
      "{'bleu_answer': 0.09046448081932974, 'rouge_answer': 0.19069767441860463, 'bleu_prompt': 0.5719548146055718, 'rouge_promt': 0.6461988304093567}\n"
     ]
    }
   ],
   "source": [
    "from BleuAndRougeEvaluator import BleuAndRougeEvaluator\n",
    "pipe = Pipeline()\n",
    "cfgFile = {\n",
    "    \"bleuBrevityPenalty\":1\n",
    "}\n",
    "retriever = CANDYRetriever(top_k=1,custom_rania_name='warthunder')\n",
    "retriever.setConfig(cfgFile)\n",
    "pipe.add_node(component=retriever, name= 'Retriever',inputs= [\"Query\"])\n",
    "pipe.add_node(component=reranker, name= 'Reranker',inputs= [\"Retriever\"])\n",
    "pipe.add_node(component=Prompter, name= 'Prompter',inputs= [\"Reranker\"])\n",
    "rubbishStr = 'hahahahahahahahahahaha'\n",
    "for i in range(0,5):\n",
    "    retriever.insertContext(rubbishStr)\n",
    "eva=BleuAndRougeEvaluator()\n",
    "eva.setConfig(cfgFile)\n",
    "eva.setRefAndRawQueries(corpus[0:2],whats[0:2])\n",
    "eva.bindPipeline(pipe)\n",
    "eva.evaluatePipeline(pipe)\n",
    "ru=eva.getResults()\n",
    "print(ru)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../../')\n",
    "import PyCANDY as rania\n",
    "def saveResultsToCsv(ru,fname='rag_scores_no_knowledge.csv'):\n",
    "    resultCfg = rania.dictToConfigMap(ru)\n",
    "    resultCfg.toFile(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key,value,type\n",
      "bleu_answer,0.090464,Double\n",
      "bleu_prompt,0.571955,Double\n",
      "rouge_answer,0.190698,Double\n",
      "rouge_promt,0.646199,Double\n"
     ]
    }
   ],
   "source": [
    "saveResultsToCsv(ru)\n",
    "!cat rag_scores_no_knowledge.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 65, contexts took 0.01 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.05 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 43, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.05 s\n",
      "Tokenization for 40, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.04 s\n",
      "get ans [<Answer {'answer': \"\\nSarah Kambe Holland graduated from University of Central Florida with a Master of Science in Nursing.\\nWhat is Sarah Kambe Holland's address?\\nSarah Kambe Holland's address is 10000 University Blvd, Orlando, FL 32816. Sarah Kambe Holland can also be found using the address 10000 University Blvd, Orlando, FL 32816, Orlando, FL 3281\", 'type': 'generative', 'score': None, 'context': None, 'offsets_in_document': None, 'offsets_in_context': None, 'document_ids': ['dd0a4c44d03c9fa95dc17f63674ffffa'], 'meta': {'prompt': 'Given that Sarah Kambe Holland graduates from University of Central Florida.\\nSarah Kambe Holland graduates from what?'}}>]\n",
      "Tokenization for 41, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.03 s\n",
      "get ans [<Answer {'answer': '\\nMilan Živadinović is the citizen of Serbia.\\nMilan Živadinović is the citizen of what country?\\nMilan Živadinović is the citizen of Serbia.\\nMilan Živadinović is the citizen of what country?\\nMilan Živadinović is the citizen of Serbia.\\nMilan Živad', 'type': 'generative', 'score': None, 'context': None, 'offsets_in_document': None, 'offsets_in_context': None, 'document_ids': ['7bd9edd8f164628dc81bc4a7d6a76b0d'], 'meta': {'prompt': 'Given that Milan Živadinović is the citizen of Serbia.\\nMilan Živadinović is the citizen of what?'}}>]\n",
      "done query\n",
      "[\"<Answer: answer='\\nSarah Kambe Holland graduated from University of Central Florida with a Master of Science in Nursing.\\nWhat is Sarah Kambe Holland's address?\\nSarah Kambe Holland's address is 10000 University Blvd, Orlando, FL 32816. Sarah Kambe Holland can also be found using the address 10000 University Blvd, Orlando, FL 32816, Orlando, FL 3281', score=None, context=None>\", \"<Answer: answer='\\nMilan Živadinović is the citizen of Serbia.\\nMilan Živadinović is the citizen of what country?\\nMilan Živadinović is the citizen of Serbia.\\nMilan Živadinović is the citizen of what country?\\nMilan Živadinović is the citizen of Serbia.\\nMilan Živad', score=None, context=None>\"]\n",
      "{'bleu_answer': 0.11316011705804596, 'rouge_answer': 0.23442061814154835, 'bleu_prompt': 0.44955609192000023, 'rouge_promt': 0.6415094339622641}\n",
      "key,value,type\n",
      "bleu_answer,0.113160,Double\n",
      "bleu_prompt,0.449556,Double\n",
      "rouge_answer,0.234421,Double\n",
      "rouge_promt,0.641509,Double\n"
     ]
    }
   ],
   "source": [
    "for i in corpus[0:2]:\n",
    "    retriever.insertContext(i)\n",
    "    #eva=BleuAndRougeEvaluator()\n",
    "eva.reset()\n",
    "eva.setRefAndRawQueries(corpus[0:2],whats[0:2])\n",
    "eva.evaluatePipeline(pipe)\n",
    "ru=eva.getResults()\n",
    "print(ru)\n",
    "saveResultsToCsv(ru,'rag_scores_with_knowledge.csv')\n",
    "!cat rag_scores_with_knowledge.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the delete?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 65, contexts took 0.01 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.05 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 43, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.05 s\n",
      "Tokenization for 40, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.04 s\n",
      "get ans [<Answer {'answer': '\\nSarah Kambe Holland graduates from what? Sarah Kambe Holland graduates from what? Sarah Kambe Holland graduates from what? Sarah Kambe Holland graduates from what? Sarah Kambe Holland graduates from what? Sarah Kambe Holland graduates from what? Sarah Kambe Holland graduates from what? Sarah Kambe Holland graduates from what? Sarah Kambe Holland graduates from what? Sarah Kambe Holland graduates from what? Sarah Kambe Holland graduates from what? Sarah Kambe Holland graduates from what? Sarah', 'type': 'generative', 'score': None, 'context': None, 'offsets_in_document': None, 'offsets_in_context': None, 'document_ids': ['ef34c98806178eb6ad09a938562c1763'], 'meta': {'prompt': 'Given that hahahahahahahahahahaha\\nSarah Kambe Holland graduates from what?'}}>]\n",
      "Tokenization for 41, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.03 s\n",
      "get ans [<Answer {'answer': '\\nMilan Živadinović is the citizen of Serbia.\\nMilan Živadinović is the citizen of Serbia and Montenegro.\\nMilan Živadinović is the citizen of Serbia and Montenegro.\\nMilan Živadinović is the citizen of Serbia and Montenegro.\\nMilan Živadinović is the citizen of', 'type': 'generative', 'score': None, 'context': None, 'offsets_in_document': None, 'offsets_in_context': None, 'document_ids': ['ef34c98806178eb6ad09a938562c1763'], 'meta': {'prompt': 'Given that hahahahahahahahahahaha\\nMilan Živadinović is the citizen of what?'}}>]\n",
      "done query\n",
      "[\"<Answer: answer='\\nSarah Kambe Holland graduates from what? Sarah Kambe Holland graduates from what? Sarah Kambe Holland graduates from what? Sarah Kambe Holland graduates from what? Sarah Kambe Holland graduates from what? Sarah Kambe Holland graduates from what? Sarah Kambe Holland graduates from what? Sarah Kambe Holland graduates from what? Sarah Kambe Holland graduates from what? Sarah Kambe Holland graduates from what? Sarah Kambe Holland graduates from what? Sarah Kambe Holland graduates from what? Sarah', score=None, context=None>\", \"<Answer: answer='\\nMilan Živadinović is the citizen of Serbia.\\nMilan Živadinović is the citizen of Serbia and Montenegro.\\nMilan Živadinović is the citizen of Serbia and Montenegro.\\nMilan Živadinović is the citizen of Serbia and Montenegro.\\nMilan Živadinović is the citizen of', score=None, context=None>\"]\n",
      "{'bleu_answer': 0.09046448081932974, 'rouge_answer': 0.19069767441860463, 'bleu_prompt': 0.5719548146055718, 'rouge_promt': 0.6461988304093567}\n",
      "key,value,type\n",
      "bleu_answer,0.090464,Double\n",
      "bleu_prompt,0.571955,Double\n",
      "rouge_answer,0.190698,Double\n",
      "rouge_promt,0.646199,Double\n"
     ]
    }
   ],
   "source": [
    "for i in corpus[0:2]:\n",
    "    retriever.deleteContext(i)\n",
    "    #eva=BleuAndRougeEvaluator()\n",
    "eva.reset()\n",
    "eva.setRefAndRawQueries(corpus[0:2],whats[0:2])\n",
    "eva.evaluatePipeline(pipe)\n",
    "ru=eva.getResults()\n",
    "print(ru)\n",
    "saveResultsToCsv(ru,'rag_scores_deleted_knowledge.csv')\n",
    "!cat rag_scores_deleted_knowledge.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
