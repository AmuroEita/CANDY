{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the tutorial to show how to evaluate the RAG's accuracy metrics by using haystack's key value store. We have a customized retriver and special domain knowledge, create the RAG under haystack pipeline, and  evaluate the RAG by using a packed evaluation class. The results are saved in standarlized csv files, for the convinience of both human and machine.\n",
    "- Dataset: warthunder key words and descriptions (pre-downloaded, last accessed on Apr 1, 2024)\n",
    "- Metrics: bleu and rouge, for both the ending results and intermediate prompts (by using haystack's K-V in evaluator classs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tony/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "2024-04-15 16:04:24,080\tINFO util.py:159 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack.nodes.prompt import PromptNode\n",
    "from haystack.nodes import PromptModel\n",
    "from haystack.nodes.prompt.prompt_template import PromptTemplate\n",
    "from haystack.nodes import AnswerParser\n",
    "from haystack.nodes.ranker import SentenceTransformersRanker\n",
    "from haystack.nodes.retriever import BM25Retriever\n",
    "from haystack.document_stores import InMemoryDocumentStore\n",
    "from haystack import Document\n",
    "from warthunder2 import warthunderRead\n",
    "from CANDYRetriever2 import CANDYRetriever\n",
    "\n",
    "keys,descs,usages,histories = warthunderRead.paraseInCategories(warthunderRead.genRencentCategories())\n",
    "strList=usages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the retriver? Do remember to config it with K-V."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tony/.local/lib/python3.10/site-packages/torch/_utils.py:832: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.weight', 'question_encoder.bert_model.pooler.dense.bias']\n",
      "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:Mon Apr 15 16:04:33 2024:/home/tony/projects/CANDY/src/CANDY/FlatAMMIPObjIndex.cpp:24|virtual bool CANDY::FlatAMMIPObjIndex::setConfig(INTELLI::ConfigMapPtr)|\u001b[34mSize of DCO=-1\u001b[0m\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 341, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 1.24 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 544, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.06 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 28, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.15 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 31, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.04 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 257, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.05 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 1151, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.10 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 1045, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.12 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 1163, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.11 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 1319, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.10 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 324, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.04 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 781, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.06 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 108, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.04 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 152, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.03 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 182, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.02 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 190, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.02 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 840, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.07 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 26, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.03 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 931, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.07 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 1432, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.10 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 368, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.04 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 842, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.07 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 248, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.03 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 202, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.03 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 877, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.07 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 1163, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.10 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 148, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.03 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 564, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.05 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 386, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.04 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 192, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.03 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 404, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.04 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 838, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.06 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 311, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.04 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 430, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.04 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 615, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.05 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 151, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.02 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 421, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.04 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 988, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.07 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 482, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.04 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 839, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.07 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 782, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.06 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 68, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.03 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 1443, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.11 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 290, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.04 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 350, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.04 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 1158, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.07 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 259, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.04 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 131, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.03 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 259, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.04 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 1727, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.13 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 998, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.07 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 1101, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.07 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 1132, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.09 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 247, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.04 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 460, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.04 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 1048, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.07 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 215, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.03 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 492, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.04 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 366, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.04 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 700, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.05 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 26, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.02 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 302, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.04 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 406, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.04 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 1611, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.11 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 409, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.04 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 128, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.03 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 1150, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.09 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 1136, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.09 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 940, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.07 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 857, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.07 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 1014, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.07 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 39, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.04 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 225, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.03 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 169, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.02 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 405, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.04 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 218, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.02 s\n"
     ]
    }
   ],
   "source": [
    "a=CANDYRetriever(top_k=1)\n",
    "a.setConfig({'ANNK':1})\n",
    "ctx = strList[100:175]\n",
    "qtx = ['What is A-6E_TRAM?']\n",
    "for i in ctx:\n",
    "    a.insertContext(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here comes the RAG part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastrag.prompters.invocation_layers.llama_cpp import LlamaCPPInvocationLayer\n",
    "from haystack import Pipeline\n",
    "from haystack.nodes.prompt import PromptNode\n",
    "from haystack.nodes import PromptModel\n",
    "from haystack.nodes.prompt.prompt_template import PromptTemplate\n",
    "from haystack.nodes import AnswerParser\n",
    "from haystack.nodes.ranker import SentenceTransformersRanker\n",
    "from haystack.nodes.retriever import BM25Retriever\n",
    "from haystack.document_stores import InMemoryDocumentStore\n",
    "from haystack import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.weight', 'question_encoder.bert_model.pooler.dense.bias']\n",
      "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "retriever = CANDYRetriever(top_k=1,custom_rania_name='warthunder')\n",
    "reranker = SentenceTransformersRanker(\n",
    "    batch_size= 32,\n",
    "    model_name_or_path= \"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
    "    top_k= 1,\n",
    "    use_gpu= False\n",
    ")\n",
    "AParser = AnswerParser()\n",
    "LFQA = PromptTemplate(\n",
    "    prompt=\"\"\"{join(documents)}\n",
    "Question: {query}\n",
    "Answer: \"\"\",\n",
    "    output_parser= AParser\n",
    ")\n",
    "PrompterModel = PromptModel(\n",
    "    model_name_or_path= 'openlm-research/open_llama_3b',\n",
    "    model_kwargs={ 'max_new_token':256}\n",
    ")\n",
    "Prompter = PromptNode(\n",
    "    model_name_or_path= PrompterModel,\n",
    "    default_prompt_template= LFQA\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the evaluator part in the following."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install packages and import the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rouge_score bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BleuAndRougeEvaluator import BleuAndRougeEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the RAG without any outside knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.weight', 'question_encoder.bert_model.pooler.dense.bias']\n",
      "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:Mon Apr 15 17:01:00 2024:/home/tony/projects/CANDY/src/CANDY/FlatAMMIPObjIndex.cpp:24|virtual bool CANDY::FlatAMMIPObjIndex::setConfig(INTELLI::ConfigMapPtr)|\u001b[34mSize of DCO=-1\u001b[0m\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 22, contexts took 0.03 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.38 s\n",
      "I can evaluate bleu and rougle\n",
      "Tokenization for 21, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.04 s\n",
      "get ans [<Answer {'answer': '1994-2000 P-26A-34_M2\\nQuestion: 2. What is the difference between the P-26A-34_M2 and the P-26A-34_M3?\\nAnswer: The P-26A-34_M2 is a 1994-2000 P-26A-34_M2. The', 'type': 'generative', 'score': None, 'context': None, 'offsets_in_document': None, 'offsets_in_context': None, 'document_ids': ['ef34c98806178eb6ad09a938562c1763'], 'meta': {'prompt': 'hahahahahahahahahahaha\\nQuestion:  What is P-26A-34_M2?\\nAnswer: '}}>]\n",
      "Tokenization for 18, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.03 s\n",
      "get ans [<Answer {'answer': '1950s US Air Force jet fighter.\\nQuestion: 2. What is the name of the 1950s US Air Force jet fighter?\\nAnswer: P-26A-33\\nQuestion: 3. What is the name of the 1950s US Air Force jet fighter?\\nAnswer: P-26A-33.\\nQuestion: 4. What is the name of', 'type': 'generative', 'score': None, 'context': None, 'offsets_in_document': None, 'offsets_in_context': None, 'document_ids': ['ef34c98806178eb6ad09a938562c1763'], 'meta': {'prompt': 'hahahahahahahahahahaha\\nQuestion:  What is P-26A-33?\\nAnswer: '}}>]\n",
      "done query\n",
      "[\"<Answer: answer='1994-2000 P-26A-34_M2\\nQuestion: 2. What is the difference between the P-26A-34_M2 and the P-26A-34_M3?\\nAnswer: The P-26A-34_M2 is a 1994-2000 P-26A-34_M2. The', score=None, context=None>\", \"<Answer: answer='1950s US Air Force jet fighter.\\nQuestion: 2. What is the name of the 1950s US Air Force jet fighter?\\nAnswer: P-26A-33\\nQuestion: 3. What is the name of the 1950s US Air Force jet fighter?\\nAnswer: P-26A-33.\\nQuestion: 4. What is the name of', score=None, context=None>\"]\n",
      "{'bleu_answer': 0.11713444302026274, 'rouge_answer': 0.31932773109243695, 'bleu_prompt': 0.004547695000302876, 'rouge_promt': 0.14538006724117944}\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline()\n",
    "cfgFile = {\n",
    "    \"bleuBrevityPenalty\":1\n",
    "}\n",
    "retriever = CANDYRetriever(top_k=1,custom_rania_name='warthunder')\n",
    "retriever.setConfig(cfgFile)\n",
    "pipe.add_node(component=retriever, name= 'Retriever',inputs= [\"Query\"])\n",
    "pipe.add_node(component=reranker, name= 'Reranker',inputs= [\"Retriever\"])\n",
    "pipe.add_node(component=Prompter, name= 'Prompter',inputs= [\"Reranker\"])\n",
    "rubbishStr = 'hahahahahahahahahahaha'\n",
    "retriever.insertContext(rubbishStr)\n",
    "eva=BleuAndRougeEvaluator()\n",
    "eva.setConfig(cfgFile)\n",
    "eva.setRefAndQueries(descs[0:2],keys[0:2])\n",
    "eva.evaluatePipeline(pipe)\n",
    "ru=eva.getResults()\n",
    "print(ru)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also save this result into a standarlized csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../../../')\n",
    "import PyCANDY as rania\n",
    "resultCfg = rania.dictToConfigMap(ru)\n",
    "resultCfg.toFile('rag_scores_no_knowledge.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's insert knowledge and see what if?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 504, contexts took 0.01 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.13 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 204, contexts took 0.01 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.05 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 22, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.04 s\n",
      "I can evaluate bleu and rougle\n",
      "Tokenization for 21, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.03 s\n",
      "get ans [<Answer {'answer': '\\nThe P-26A-34\\xa0Peashooter is a rank I American fighter \\nwith a battle rating of 1.0 (AB/RB/SB). It has been in the game since the start of the Open Beta Test prior to Update 1.27.\\n\\nQuestion: \\nWhat is P-26A-35?\\nAnswer: \\nThe P-26A-35\\xa0Peash', 'type': 'generative', 'score': None, 'context': None, 'offsets_in_document': None, 'offsets_in_context': None, 'document_ids': ['3ede1fa8ecfcc7ac01bc7098db072bf0'], 'meta': {'prompt': 'Description of P-26A-33: \\nThe P-26A-33\\xa0Peashooter is a rank I American fighter \\nwith a battle rating of 1.0 (AB/RB/SB). It has been in the game since the start of the Open Beta Test prior to Update 1.27.\\n\\nQuestion:  What is P-26A-34_M2?\\nAnswer: '}}>]\n",
      "Tokenization for 18, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.07 s\n",
      "get ans [<Answer {'answer': '\\nThe P-26A-33\\xa0Peashooter is a rank I American fighter \\nwith a battle rating of 1.0 (AB/RB/SB). It has been in the game since the start of the Open Beta Test prior to Update 1.27.\\n\\nQuestion: \\nWhat is the difference between P-26A-33 and P-26A-33A?\\nAnswer: ', 'type': 'generative', 'score': None, 'context': None, 'offsets_in_document': None, 'offsets_in_context': None, 'document_ids': ['3ede1fa8ecfcc7ac01bc7098db072bf0'], 'meta': {'prompt': 'Description of P-26A-33: \\nThe P-26A-33\\xa0Peashooter is a rank I American fighter \\nwith a battle rating of 1.0 (AB/RB/SB). It has been in the game since the start of the Open Beta Test prior to Update 1.27.\\n\\nQuestion:  What is P-26A-33?\\nAnswer: '}}>]\n",
      "done query\n",
      "[\"<Answer: answer='\\nThe P-26A-34\\xa0Peashooter is a rank I American fighter \\nwith a battle rating of 1.0 (AB/RB/SB). It has been in the game since the start of the Open Beta Test prior to Update 1.27.\\n\\nQuestion: \\nWhat is P-26A-35?\\nAnswer: \\nThe P-26A-35\\xa0Peash', score=None, context=None>\", \"<Answer: answer='\\nThe P-26A-33\\xa0Peashooter is a rank I American fighter \\nwith a battle rating of 1.0 (AB/RB/SB). It has been in the game since the start of the Open Beta Test prior to Update 1.27.\\n\\nQuestion: \\nWhat is the difference between P-26A-33 and P-26A-33A?\\nAnswer: ', score=None, context=None>\"]\n",
      "{'bleu_answer': 0.37165729562970834, 'rouge_answer': 0.5332488006379168, 'bleu_prompt': 0.4666667089177204, 'rouge_promt': 0.628113365245802}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline()\n",
    "pipe.add_node(component=retriever, name= 'Retriever',inputs= [\"Query\"])\n",
    "pipe.add_node(component=reranker, name= 'Reranker',inputs= [\"Retriever\"])\n",
    "pipe.add_node(component=Prompter, name= 'Prompter',inputs= [\"Reranker\"])\n",
    "for i in descs[0:2] :\n",
    "    retriever.insertContext(i)\n",
    "rubbishStr = 'hahahahahahahahahahaha'\n",
    "retriever.insertContext(rubbishStr)\n",
    "eva=BleuAndRougeEvaluator()\n",
    "eva.setConfig({\"bleuBrevityPenalty\":1})\n",
    "eva.setRefAndQueries(descs[0:2],keys[0:2])\n",
    "eva.evaluatePipeline(pipe)\n",
    "ru=eva.getResults()\n",
    "print(ru)\n",
    "resultCfg = rania.dictToConfigMap(ru)\n",
    "resultCfg.toFile('rag_scores_with_knowledge.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the csv files for the differences!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we delete the knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 504, contexts took 0.02 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.08 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 204, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.06 s\n"
     ]
    }
   ],
   "source": [
    "for i in descs[0:2] :\n",
    "    retriever.deleteContext(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I can evaluate bleu and rougle\n",
      "Tokenization for 21, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.05 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tony/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get ans [<Answer {'answer': '1994-2000 P-26A-34_M2\\nQuestion: 2. What is the difference between the P-26A-34_M2 and the P-26A-34_M3?\\nAnswer: The P-26A-34_M2 is a 1994-2000 P-26A-34_M2. The', 'type': 'generative', 'score': None, 'context': None, 'offsets_in_document': None, 'offsets_in_context': None, 'document_ids': ['ef34c98806178eb6ad09a938562c1763'], 'meta': {'prompt': 'hahahahahahahahahahaha\\nQuestion:  What is P-26A-34_M2?\\nAnswer: '}}>]\n",
      "Tokenization for 18, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.03 s\n",
      "get ans [<Answer {'answer': '1950s US Air Force jet fighter.\\nQuestion: 2. What is the name of the 1950s US Air Force jet fighter?\\nAnswer: P-26A-33\\nQuestion: 3. What is the name of the 1950s US Air Force jet fighter?\\nAnswer: P-26A-33.\\nQuestion: 4. What is the name of', 'type': 'generative', 'score': None, 'context': None, 'offsets_in_document': None, 'offsets_in_context': None, 'document_ids': ['ef34c98806178eb6ad09a938562c1763'], 'meta': {'prompt': 'hahahahahahahahahahaha\\nQuestion:  What is P-26A-33?\\nAnswer: '}}>]\n",
      "done query\n",
      "[\"<Answer: answer='1994-2000 P-26A-34_M2\\nQuestion: 2. What is the difference between the P-26A-34_M2 and the P-26A-34_M3?\\nAnswer: The P-26A-34_M2 is a 1994-2000 P-26A-34_M2. The', score=None, context=None>\", \"<Answer: answer='1950s US Air Force jet fighter.\\nQuestion: 2. What is the name of the 1950s US Air Force jet fighter?\\nAnswer: P-26A-33\\nQuestion: 3. What is the name of the 1950s US Air Force jet fighter?\\nAnswer: P-26A-33.\\nQuestion: 4. What is the name of', score=None, context=None>\"]\n",
      "{'bleu_answer': 0.11713444302026274, 'rouge_answer': 0.31932773109243695, 'bleu_prompt': 0.004547695000302876, 'rouge_promt': 0.14538006724117944}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline()\n",
    "pipe.add_node(component=retriever, name= 'Retriever',inputs= [\"Query\"])\n",
    "pipe.add_node(component=reranker, name= 'Reranker',inputs= [\"Retriever\"])\n",
    "pipe.add_node(component=Prompter, name= 'Prompter',inputs= [\"Reranker\"])\n",
    "eva=BleuAndRougeEvaluator()\n",
    "eva.setConfig({\"bleuBrevityPenalty\":1})\n",
    "eva.setRefAndQueries(descs[0:2],keys[0:2])\n",
    "eva.evaluatePipeline(pipe)\n",
    "ru=eva.getResults()\n",
    "print(ru)\n",
    "resultCfg = rania.dictToConfigMap(ru)\n",
    "resultCfg.toFile('rag_scores_deleted_knowledge.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the csv file, and LLM becomes nerd again!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
